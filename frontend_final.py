import streamlit as st
import pandas as pd
import numpy as np
import os
import joblib
from sklearn.metrics.pairwise import cosine_similarity
from langdetect import detect
from googletrans import Translator
from streamlit_option_menu import option_menu
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from deep_translator import GoogleTranslator

import json
import re
import nltk
import joblib
import requests
import streamlit as st

import gdown
from nltk.corpus import stopwords
from nltk.stem import WordNetLemmatizer
from sklearn.feature_extraction.text import TfidfVectorizer
from sklearn.decomposition import TruncatedSVD
from sklearn.metrics.pairwise import cosine_similarity
from deep_translator import GoogleTranslator

st.set_page_config(layout="wide", page_icon="üé¨", page_title="LSA")

# Fichiers sur Google Drive
FILES = {
    "tfidf_vectorizer.joblib": "1xiTMlCp_ceoMYJ68GIBz-MErDCoQVZYI",
    "lsa_model.joblib": "1H3qa870LcA7noI7HEhx9ExBHFzgIDbmh",
    "lsa_data.npz": "1w-RrDtZbHgllJXHhO9bWis75wAkoo0rH"
}

# T√©l√©chargement auto si fichier absent
def download_all_artifacts():
    for filename, file_id in FILES.items():
        if not os.path.exists(filename):
            url = f"https://drive.google.com/uc?id={file_id}"
            st.info(f"T√©l√©chargement de {filename} en cours‚Ä¶")
            gdown.download(url, filename, quiet=False)
        else:
            print(f"{filename} d√©j√† pr√©sent. T√©l√©chargement ignor√©.")

# Appel du t√©l√©chargement
download_all_artifacts()

# Chemins
VECTORIZER_PATH = "tfidf_vectorizer.joblib"
LSA_MODEL_PATH  = "lsa_model.joblib"
LSA_DATA_PATH   = "lsa_data.npz"

# Traduction
translator = Translator()


# Chargement des artefacts
@st.cache_resource
def load_lsa_artifacts():
    for f in [VECTORIZER_PATH, LSA_MODEL_PATH, LSA_DATA_PATH]:
        if not os.path.isfile(f):
            st.error(f"Artefact manquant : {f}")
            st.stop()
    vec = joblib.load(VECTORIZER_PATH)
    lsa = joblib.load(LSA_MODEL_PATH)
    data = np.load(LSA_DATA_PATH, allow_pickle=True)
    return vec, lsa, data["X_lsa"], data["titles"]

vectorizer, lsa_model, X_lsa, titles_ref = load_lsa_artifacts()

def recommend_general(user_title: str, top_n: int = 10):
    tfidf_ut = vectorizer.transform([user_title])
    lsa_ut = lsa_model.transform(tfidf_ut)
    sims = cosine_similarity(lsa_ut, X_lsa).flatten()
    idxs = np.argsort(sims)[::-1][:top_n]
    return [(titles_ref[i], sims[i]) for i in idxs]


# CSS pour am√©liorer l'apparence
st.markdown("""
    <style>
        /* Titre principal */
        h1 {
            font-family: 'Arial', sans-serif;
            color: #1f77b4;
            font-size: 40px;
            text-align: center;
            font-weight: bold;
        }
        
        /* Sous-titres */
        h2, h3 {
            font-family: 'Poppins', sans-serif;
            color: #ff6f61;
            font-size: 25px;
            font-weight: bold;
        }
        
        /* Texte g√©n√©ral */
        p, li, span {
            font-family: 'Nunito SemiBold', sans-serif;
            color: #333;
            font-size: 13x;
            line-height: 1.6;
        }
        
        /* Texte dans la barre lat√©rale */
        .sidebar .sidebar-content {
            font-family: 'Poppins', sans-serif;
            color: #4d4d4d;
        }

        /* Couleur des liens */
        a {
            color: #0073e6;
            text-decoration: none;
        }

        /* Ajouter un survol aux liens */
        a:hover {
            color: #ff6f61;
        }
        
        /* Barre de progression */
        .stProgress {
            height: 20px;
        }
    </style>
""", unsafe_allow_html=True)

# CSS
st.markdown("""
<style>
body, .main { background-color: #F8F8FF}
.header {
  background-color: #8B4513; padding: 15px 0; border-radius: 8px;
  color: white; margin-bottom: 20px;
}
.header .logo-left, .header .logo-right { width: 180px !important; margin: auto; }
.header .title { text-align: center; font-size: 2.5rem; margin: 0; line-height: 180px; }
.header:after { content: ""; display: block; clear: both; }
.team-card { display: flex; align-items: center; margin-bottom: 10px; }
.team-card .indicator {
  width: 12px; height: 12px; background-color: #4B0082;
  margin-right: 10px; border-radius: 2px;
}
.team-card .info { font-size: 1.1rem; }
.stButton>button {
  background-color: #4B0082; color: white; border-radius: 5px;
  padding: 0.5em 1.5em;
}
.stButton>button:hover { background-color: #6A0DAD; }
.section {
  background-color: #FFFFFF; padding: 15px; border-radius: 8px;
  box-shadow: 1px 1px 4px rgba(0,0,0,0.1); margin-bottom: 20px;
}
</style>
""", unsafe_allow_html=True)

# Barre lat√©rale de navigation
#page = st.sidebar.selectbox("Navigation", ["üè† Accueil", "üìΩÔ∏è Mod√®le"])

# === PAGE ACCUEIL ========================================================================
#if page == "üè† Accueil":
def page_accueil():
    st.markdown("<div class='header'>", unsafe_allow_html=True)
    col_l, col_t, col_r = st.columns([1, 7, 1])
    with col_l:
        st.image("ise.jpg", width=160)
    with col_t:
        st.markdown('<h1 class="title">Recommandation de Films LSA</h1>', unsafe_allow_html=True)
    with col_r:
        st.image("eneam.jpg", width=160)
    st.markdown("</div>", unsafe_allow_html=True)

    with st.expander("üìò Bienvenue", expanded=True):
        st.markdown("""<h3 style='font-family: "Trebuchet MS", sans-serif; font-weight: bold; color: #B8860B;'>
        Bienvenue sur l'interface de recommandation de films par similarit√© s√©mantique (mod√®le LSA)
        </h3>""", unsafe_allow_html=True)
        st.markdown("""
        1. üé• Entrez un **titre de film**
        2. üåê Choisissez sa **langue** (en / fr)
        3. üîé Cliquez sur **Recommander**
        4. üìä Explorez les 10 films les plus similaires
        5. üíæ T√©l√©chargez les r√©sultats
        """)

    st.markdown("<div class='section'>", unsafe_allow_html=True)
    st.markdown("### üë• Membres du groupe")
    members = [
        {"nom": "Peace AHOUANSE",     "r√¥le": "Data Scientist"},
        {"nom": "Brilland BABA",      "r√¥le": "Front-end"},
        {"nom": "Ez√©chiel DESSOUASSI","r√¥le": "Ing√©nieur ML"},
        {"nom": "T√©rence KPADADONOU", "r√¥le": "Back-end"},
    ]
    col_a, col_b = st.columns(2)
    for idx, col in enumerate([col_a, col_b]):
        with col:
            for m in members[idx*2:(idx+1)*2]:
                st.markdown(f"""
                <div class="team-card">
                  <div class="indicator"></div>
                  <div class="info"><strong>{m["nom"]}</strong> ‚Äî {m["r√¥le"]}</div>
                </div>""", unsafe_allow_html=True)
    st.markdown("</div>", unsafe_allow_html=True)
    
    st.markdown("<div class='section'>", unsafe_allow_html=True)

    st.title("üß† Description du Mod√®le de Recommandation")

    st.header("üéØ 1. Objectif du Projet")
    st.markdown("""
    L‚Äôobjectif principal est de concevoir un syst√®me de recommandation de films capable de proposer des titres similaires √† une requ√™te utilisateur, en s‚Äôappuyant sur l‚Äôanalyse s√©mantique des contenus textuels (r√©sum√©s, slogans, genres).  
    Le syst√®me est **multilingue**, int√®gre la d√©tection et traduction de langues, et mesure la performance via des indicateurs de **pr√©cision**.
    """)

    st.header("üìä 2. Nature et Source des Donn√©es")
    st.markdown("""
    Les donn√©es sont issues d‚Äôune base t√©l√©charg√©e depuis **Kaggle** : `movies_metadata.csv`.  
    Cette base contient des **m√©tadonn√©es sur les films**, dont seuls les champs essentiels ont √©t√© retenus :
    - `original_title`
    - `overview`
    - `genres`
    """)

    st.header("üßπ 3. Chargement et Pr√©paration des Donn√©es")
    st.markdown("""
    Le processus commence par un nettoyage :
    - Extraction structur√©e des genres (depuis une cha√Æne JSON)
    - Nettoyage des r√©sum√©s : minuscules, ponctuation, mots vides, lemmatisation

    > üîé Ce nettoyage est **essentiel** pour garantir la qualit√© de l‚Äôanalyse s√©mantique.
    """)

    st.header("üßÆ 4. Construction du Mod√®le LSA (Latent Semantic Analysis)")
    st.subheader("üì• a. V√©rification des Donn√©es Sources")
    st.markdown("Une v√©rification pr√©alable garantit que les donn√©es pr√©trait√©es sont bien pr√©sentes et fiables.")

    st.subheader("üßæ b. Vectorisation via TF-IDF")
    st.markdown("""
    Les r√©sum√©s sont transform√©s en vecteurs num√©riques selon le sch√©ma **TF-IDF** :
    - Renforce les mots rares mais significatifs
    - R√©duit l'impact des mots tr√®s fr√©quents et peu informatifs
    """)

    st.subheader("üîª c. R√©duction de Dimension via LSA (SVD)")
    st.markdown("""
    Une **d√©composition en valeurs singuli√®res (SVD)** est appliqu√©e √† la matrice TF-IDF :
    - R√©duit la dimension de l‚Äôespace
    - R√©v√®le les **structures th√©matiques sous-jacentes**
    - Permet de **projeter** les films dans un espace s√©mantique r√©duit
    """)

    st.subheader("üíæ d. Sauvegarde des Objets Mod√©lis√©s")
    st.markdown("Les objets TF-IDF, SVD, corpus vectoris√©s et titres sont **s√©rialis√©s** pour un rechargement rapide.")

    st.subheader("üì° e. Mise en Place du Syst√®me de Recommandation")
    st.markdown("""
    Le syst√®me fonctionne en trois √©tapes :
    1. Traduction (si besoin)
    2. Vectorisation du texte utilisateur
    3. Calcul des **similarit√©s cosinus** avec les films du corpus

    ‚Üí Les films les plus proches sont retourn√©s avec un **score de similarit√©**.
    """)

    st.subheader("‚úÖ f. Tests de Validation")
    st.markdown("Des tests ont √©t√© r√©alis√©s avec des **titres connus** pour √©valuer la coh√©rence des suggestions.")

    st.header("üìò 5. Pr√©sentation de la M√©thode LSA")
    st.markdown("""
    La **Latent Semantic Analysis (LSA)** est une m√©thode NLP qui :
    - R√©v√®le les **relations s√©mantiques latentes**
    - Se base sur une **d√©composition SVD** de la matrice TF-IDF
    """)

    st.subheader("üßæ a. Constitution de la Matrice TF-IDF")
    st.markdown("""
    - Chaque ligne = un mot du vocabulaire  
    - Chaque colonne = une description de film  
    - Chaque cellule = poids TF-IDF du mot dans le film
    """)

    st.subheader("üîª b. R√©duction avec la SVD")
    st.latex(r"A \approx U_k \Sigma_k V_k^T")
    st.markdown("""
    - A : matrice TF-IDF  
    - U_k : vecteurs propres des termes  
    - Œ£_k : valeurs singuli√®res  
    - V_k^T : vecteurs propres des documents  
    ‚Üí **On garde k composantes principales** pour capter l‚Äôessentiel de l‚Äôinformation s√©mantique.
    """)

    st.subheader("üß≠ c. Interpr√©tation de l‚ÄôEspace LSA")
    st.markdown("""
    - Documents proches = contenus similaires  
    - Mots proches = contextes s√©mantiques proches  
    - Recommandation robuste m√™me en l‚Äôabsence de termes communs
    """)

    st.success("Cette m√©thode permet une **recommandation s√©mantique intelligente**, bien plus pertinente qu‚Äôun simple appariement de mots-cl√©s.")


# === PAGE MOD√àLE =========================================================================
def modele ():
    st.markdown("<div class='section'>", unsafe_allow_html=True)
    st.markdown("### üîç Recherche")
    user_input = st.text_input("üé• Titre du film", placeholder="e.g. Titanic")
    lang = st.selectbox("üåê Langue", ["en", "fr"])
    rec_button = st.button("Recommander")
    st.markdown("</div>", unsafe_allow_html=True)

    if rec_button:
        if not user_input.strip():
            st.warning("Veuillez saisir un titre de film.")
        else:
            with st.spinner("Recherche en cours‚Ä¶"):
                try:
                    src = detect(user_input)
                    if src != lang:
                        query = translator.translate(user_input, src=src, dest=lang).text
                    else:
                        query = user_input
                except:
                    query = user_input
                raw = recommend_general(query, top_n=10)
                df_res = pd.DataFrame(raw, columns=["Titre", "Score de similarit√©"])
                df_res["Score de similarit√©"] = df_res["Score de similarit√©"].map(lambda x: f"{x:.3f}")

            st.markdown("<div class='section'>", unsafe_allow_html=True)
            st.success(f"Top 10 recommandations pour ¬´ {user_input} ¬ª")
            st.table(df_res)

            col_csv, col_xlsx, _ = st.columns(3)
            with col_csv:
                st.download_button("üíæ CSV", data=df_res.to_csv(index=False).encode('utf-8'),
                                   file_name="recommandations.csv", mime="text/csv")
            with col_xlsx:
                from io import BytesIO
                buffer = BytesIO()
                df_res.to_excel(buffer, index=False, sheet_name="Recommandations")
                buffer.seek(0)
                st.download_button("üìä Excel", data=buffer, file_name="recommandations.xlsx",
                                   mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet")
            st.markdown("</div>", unsafe_allow_html=True)



# Pied de page
st.markdown("---")
st.markdown("¬© 2025 Projet LSA ‚Ä¢ ISE 2 ENEAM ‚Ä¢ Tous droits r√©serv√©s")


with st.sidebar:
    selected_page = option_menu(
        "Menu Principal",  # Titre du menu
        ["Accueil", "Mod√®le", "Classification par genres","OOb"],  # Noms des pages
        icons=['house', 'play', 'film','play'],  # Ic√¥nes des pages
        menu_icon="cast",  # Ic√¥ne du menu principal
        default_index=0,  # Page par d√©faut s√©lectionn√©e
    )


def modele_b():
    import os
    import joblib
    import numpy as np
    import pandas as pd
    import requests
    import streamlit as st
    from langdetect import detect
    from sklearn.metrics.pairwise import cosine_similarity
    from googletrans import Translator

    # Constantes pour les chemins
    VECTORIZER_PATH = "tfidf_vectorizer.joblib"
    LSA_MODEL_PATH  = "lsa_model.joblib"
    LSA_DATA_PATH   = "lsa_data.npz"

    # 7. Recherche
    st.markdown("<div class='section'>", unsafe_allow_html=True)
    st.markdown("### üîç Recherche")
    user_input = st.text_input("üé• Titre du film", placeholder="e.g. Titanic")
    lang = st.selectbox("üåê Langue", ["en", "fr"])
    rec_button = st.button("Recommander")
    st.markdown("</div>", unsafe_allow_html=True)

    # 8. Chargement des artefacts LSA
    @st.cache_resource
    def load_lsa_artifacts():
        for f in [VECTORIZER_PATH, LSA_MODEL_PATH, LSA_DATA_PATH]:
            if not os.path.isfile(f):
                st.error(f"Artefact manquant : {f}")
                st.stop()
        vec = joblib.load(VECTORIZER_PATH)
        lsa = joblib.load(LSA_MODEL_PATH)
        data = np.load(LSA_DATA_PATH, allow_pickle=True)
        return vec, lsa, data["X_lsa"], data["titles"]

    vectorizer, lsa_model, X_lsa, titles_ref = load_lsa_artifacts()
    translator = Translator()

    # 9. R√©cup√©rer l'URL du poster via TMDb
    TMDB_API_KEY = "e63f0c5b3c1b67fc1b56421f3a0172c2"
    def get_poster_url(title: str) -> str:
        try:
            resp = requests.get(
                "https://api.themoviedb.org/3/search/movie",
                params={"api_key": TMDB_API_KEY, "query": title},
                timeout=5
            ).json()
            if resp.get("results"):
                path = resp["results"][0].get("poster_path")
                if path:
                    return "https://image.tmdb.org/t/p/w200" + path
        except:
            pass
        return ""

    # 10. Fonction de recommandation
    def recommend_general(user_title: str, top_n: int = 10):
        try:
            src = detect(user_title)
            if src != lang:
                ut = translator.translate(user_title, src=src, dest=lang).text
            else:
                ut = user_title
        except:
            ut = user_title

        tfidf_ut = vectorizer.transform([ut])
        lsa_ut = lsa_model.transform(tfidf_ut)
        sims = cosine_similarity(lsa_ut, X_lsa).flatten()
        idxs = np.argsort(sims)[::-1][:top_n]

        rows = []
        for i in idxs:
            title_i = titles_ref[i]
            score_i = sims[i]
            poster = get_poster_url(title_i)
            rows.append({
                "Poster": f"![]({poster})" if poster else "",
                "Titre": title_i,
                "Score de similarit√©": f"{score_i:.3f}",
                "PosterURL": poster
            })
        return pd.DataFrame(rows)

    # 11. Affichage & export
    if rec_button:
        if not user_input.strip():
            st.warning("Veuillez saisir un titre de film.")
        else:
            with st.spinner("Recherche en cours‚Ä¶"):
                df_res = recommend_general(user_input, top_n=10)

            st.markdown("<div class='section'>", unsafe_allow_html=True)
            st.success(f"Top 10 recommandations pour ¬´ {user_input} ¬ª")

            st.write(df_res.to_markdown(index=False), unsafe_allow_html=True)

            st.markdown("### üì• T√©l√©charger les recommandations")
            col_csv, col_xlsx, col_json = st.columns(3)

            with col_csv:
                st.download_button(
                    "üíæ CSV",
                    data=df_res.drop(columns=["Poster"]).to_csv(index=False).encode("utf-8"),
                    file_name="recommandations.csv",
                    mime="text/csv",
                )
            with col_xlsx:
                from io import BytesIO
                buf = BytesIO()
                df_res.drop(columns=["Poster"]).to_excel(buf, index=False, sheet_name="Reco")
                buf.seek(0)
                st.download_button(
                    "üìä Excel",
                    data=buf,
                    file_name="recommandations.xlsx",
                    mime="application/vnd.openxmlformats-officedocument.spreadsheetml.sheet",
                )
            with col_json:
                st.download_button(
                    "üóÑ JSON",
                    data=df_res.drop(columns=["Poster"]).to_json(orient="records", force_ascii=False).encode("utf-8"),
                    file_name="recommandations.json",
                    mime="application/json",
                )

            st.markdown("</div>", unsafe_allow_html=True)

def modele2():
    # =============================================================================
    # Application Streamlit : Recommandation de films (overview + genres + images)
    # =============================================================================


    
   # nltk.download("stopwords")
    #nltk.download("wordnet")

    # === Param√®tres
    TMDB_API_KEY = "e63f0c5b3c1b67fc1b56421f3a0172c2"

    # === Chargement des donn√©es
    @st.cache_data
    def load_data():
        df0 = pd.read_csv("movies_metadata.csv", low_memory=False)
        df = df0[["original_title", "overview", "genres"]].copy()

        def extract_genres(genre_str):
            try:
                js = genre_str.replace("'", '"')
                items = json.loads(js)
                return "|".join([g["name"] for g in items if "name" in g])
            except:
                return ""

        df["genres"] = df["genres"].astype(str).apply(extract_genres)
        df = df[df["genres"].str.strip() != ""].dropna(subset=["overview"]).reset_index(drop=True)

        stop_en = set(stopwords.words("english"))
        lemm = WordNetLemmatizer()

        def clean_text(text):
            text = re.sub(r"[^a-z0-9\s]", " ", str(text).lower())
            tokens = [lemm.lemmatize(w) for w in text.split() if w not in stop_en and len(w) > 2]
            return " ".join(tokens)

        df["clean_overview"] = df["overview"].apply(clean_text)

        return df

    df = load_data()

    # === Mod√®le TF-IDF + LSA
    @st.cache_resource
    def build_model(df):
        corpus = df["clean_overview"].tolist()
        titles = df["original_title"].tolist()

        tfidf = TfidfVectorizer(max_df=0.8, min_df=5, sublinear_tf=True)
        X_tfidf = tfidf.fit_transform(corpus)

        lsa = TruncatedSVD(n_components=100, random_state=42)
        X_lsa = lsa.fit_transform(X_tfidf)

        return tfidf, lsa, X_lsa, titles

    tfidf, lsa, X_lsa, titles = build_model(df)

    # === Fonction pour afficher les affiches
    def get_movie_poster_url(title):
        try:
            url = "https://api.themoviedb.org/3/search/movie"
            params = {"api_key": TMDB_API_KEY, "query": title}
            response = requests.get(url, params=params)
            data = response.json()
            if data["results"]:
                poster_path = data["results"][0].get("poster_path")
                if poster_path:
                    return "https://image.tmdb.org/t/p/w500" + poster_path
            return None
        except:
            return None

    # === Recommandation sans filtre
    def recommend_no_filter(user_input, top_n=5, lang="fr"):
        if lang == "fr":
            translated = GoogleTranslator(source="fr", target="en").translate(user_input)
        else:
            translated = user_input

        vect_input = tfidf.transform([translated])
        lsa_input = lsa.transform(vect_input)
        sims = cosine_similarity(lsa_input, X_lsa).flatten()
        idxs = np.argsort(sims)[::-1]

        results = []
        for i in idxs[:top_n]:
            results.append((titles[i], df.iloc[i]["genres"], sims[i]))
        return results

    # === Recommandation avec filtre
    def recommend_with_filter(user_input, genre_filter, top_n=5, lang="fr"):
        if lang == "fr":
            translated = GoogleTranslator(source="fr", target="en").translate(user_input)
        else:
            translated = user_input

        vect_input = tfidf.transform([translated])
        lsa_input = lsa.transform(vect_input)
        sims = cosine_similarity(lsa_input, X_lsa).flatten()
        idxs = np.argsort(sims)[::-1]

        results = []
        count = 0
        for i in idxs:
            genres_i = str(df.iloc[i]["genres"]).lower()
            if genre_filter.lower() in genres_i:
                results.append((titles[i], df.iloc[i]["genres"], sims[i]))
                count += 1
                if count >= top_n:
                    break
        return results

    # === Interface Streamlit
    st.title("üé¨ Recommandation de films (TF-IDF + LSA)")
    st.markdown("Obtenez des suggestions bas√©es sur les **descriptions** et **genres** de films.")

    # Choix utilisateur
    lang = st.selectbox("Langue de recherche :", ["fr", "en"], index=0)
    mode = st.radio("Mode de recherche :", ["Sans filtre de genre", "Avec filtre de genre"])

    query = st.text_input("Entrez un th√®me ou une description de film :", "robots et espace")
    top_n = st.slider("Nombre de recommandations :", 1, 10, 5)

    if mode == "Avec filtre de genre":
        genre_choices = sorted(set(g for row in df["genres"] for g in row.split("|")))
        genre_filter = st.selectbox("Choisissez un genre :", genre_choices)
    else:
        genre_filter = None

    # Recherche
    if st.button("üîç Rechercher"):
        st.info("Recherche en cours...")
        if mode == "Sans filtre de genre":
            results = recommend_no_filter(query, top_n, lang)
        else:
            results = recommend_with_filter(query, genre_filter, top_n, lang)

        if not results:
            st.warning("Aucun film trouv√©.")
        else:
            for title, genre, score in results:
                st.subheader(f"{title} (score : {score:.3f})")
                st.markdown(f"**Genres :** {genre}")
                poster_url = get_movie_poster_url(title)
                if poster_url:
                    st.image(poster_url, width=250)
                else:
                    st.text("Image non disponible.")


if selected_page == "Accueil":
    page_accueil()
elif selected_page == "Mod√®le":
    modele()
elif selected_page == "Classification par genres":
    modele2()
elif selected_page=="OOb":
    modele_b()

    